\begin{problem}{1}
    Let $\Pr$ be the probability distribution of the sequence of $n$ Bernoulli trials, $\omega = (\omega_1,\ldots,\omega_n)$, $\omega_i = 1$ or 0 with probabilities $p$ and $1-p$. Find $\Pr[\omega_1|\omega_1+\ldots+\omega_n=m]$.
\end{problem}
\begin{solution}
We directly apply the definition of conditional probability:
\begin{align*}
    \Pr[\omega_1 = 1 | \omega_1 + \ldots + \omega_n = m] &= \frac{\Pr[\omega_1 = 1 \cap \omega_1 + \ldots + \omega_n = m]}{\Pr[\omega_1 + \ldots + \omega_n = m]}, \\
    &= \frac{\binom{n-1}{m-1}p^m(1-p)^{n-m}}{\binom{n}{m}p^m(1-p)^{n-m}}, \\
    &= \binom{n-1}{m-1} \big/ \binom{n}{m}, \\
    &= \frac{m}{n}.
\end{align*}
\end{solution}
\begin{problem}{2}
    Find the distribution function of a random variable $\xi$ which takes positive values and satisfies $\Pr[\xi > x + y | \xi > x] = \Pr[\xi > y]$ for all $x,y> 0$. 
\end{problem}
\begin{solution}
    We are looking for a distribution that is \textit{memoryless}, both the exponential distribution and the Poisson distribution satisfy this property. Let $\xi$ be a exponentially-distributed random variable, parameterized by $\lambda$. Then, 
    $\Pr[\xi > x] = e^{-\lambda x}$ and 
    \begin{align*}
        \Pr[\xi > x+y | \xi > x] &= \frac{\Pr[\xi > x+y]}{\Pr[\xi > x]}, \\
        &= \frac{e^{-\lambda(x+y)}}{e^{-\lambda x}}, \\
        &= e^{-\lambda y}.
    \end{align*}
\end{solution}
\begin{problem}{3}
    Two coins are in a bag. One is symmetric, while the other is not --- if tossed it lands heads up with probability equal to 0.6. One coin is randomly pulled out of the bag and tossed. It lands heads up. What is the probability that the same coin will land heads up if tossed again?
\end{problem}
\begin{solution}
    Let $H^1, H^2$ denote the event that heads appears on the first and second toss, respectively. Then,
    \begin{align*}
        \Pr[H^2|H^1] &= \frac{\Pr[H^1 \cap H^2]}{\Pr[H^1]}, \\
        &= \frac{1/2(1/2^2) + 1/2(0.6^2)}{1/2(1/2) + 1/2(0.6)}, \\
        &= 0.55.
    \end{align*}
\end{solution}
\begin{problem}{4}
    Suppose that each of the random variables $\xi$ and $\eta$ takes at most two values $a,b$. Prove that $\xi$ and $\eta$ are independent if $\Exp{\xi\eta} = \Exp{\xi}\Exp{\eta}$. 
\end{problem}
\begin{solution}
    Suppose that $\xi,\eta$ are two random variables such that both of their images is $\{a,b\}$. We show then that if $\Exp{\xi\eta} = \Exp{\xi}\Exp{\eta}$, then $\eta$ and $\xi$ are independent. Let $p,q$ be the probability that $\xi = a,\eta=a$, respectively. Then, 
    \begin{align*}
        \Exp{\xi}\Exp{\eta} &= (pa + (1-p)b)(qa + (1-q)b), \\
        &= a^2pq + ab(p)(1-q) + ba(q)(1-p) + b^2(1-p)(1-q), \\
        \Exp{\xi\eta} &= a\Pr[\eta=a,\xi=a] + ab\Pr[\eta=a,\xi=b] + ba\Pr[\eta=b,\xi=a] + b^2\Pr[\xi=b,\eta=b],
    \end{align*}
    This implies that 
    \begin{align*}
        \Pr[\xi=a,\eta=a] &= pq = \Pr[\xi=a]\Pr[\eta=a], \\
        \Pr[\xi=b,\eta=b] &= (1-q)(1-p) = \Pr[\xi=b]\Pr[\eta=b].
    \end{align*}
    By Lemma 4.3, then events $\xi = a, \eta = b$ and $\eta=a,\xi=b$ are also independent. 
\end{solution}
\begin{problem}{5}
    Give an example of three events $A_1, A_2, A_3$ which are not independent, yet pairwise independent.
\end{problem}
\begin{solution}
    We provide two examples which are credited to S.N. Bernstein\footnote{Bernstein, S. (1946) The Theory of Probabilities. Gostechizdat, Moscow, p. 348.}. Let $\Omega = \{000, 110, 101, 011\}$, $\cF = 2^\Omega$, and $\Pr$ be the probability measure that assigns to each elementary outcome the probability $\frac{1}{4}$. Let $A_i$ be the event that the $i$\textsuperscript{th} position is a 1. The events are clearly pairwise independent:
    \begin{align}
        \Pr[A_1 \cap A_2] &= \frac{1}{4} = \Pr[A_1]\Pr[A_2], \label{eq:indep1}\\
        \Pr[A_1 \cap A_3] &= \frac{1}{4} = \Pr[A_1]\Pr[A_3], \label{eq:indep2}\\
        \Pr[A_2 \cap A_3] &= \frac{1}{4} = \Pr[A_2]\Pr[A_3]. \label{eq:indep3}
    \end{align}
    However, they are clearly not independent since $\Pr[A_1\cap A_2\cap A_3] = 0$ and the product of their probabilities is nonzero. Here, the counterexample relies on the fact that the intersection of the events is empty. Next, we demonstrate another counterexample where the intersection of the events is nonempty. Let $A_i$ denote the event that the $i$\textsuperscript{th} position is a 0. Clearly, Eq.~\ref{eq:indep1}-\ref{eq:indep3} still hold. However, $\Pr[A_1 \cap A_2 \cap A_3] = \frac{1}{4} \neq \Pr[A_1]\Pr[A_2]\Pr[A_3]$. 
    Interestingly, Bernstein's counterexample to this converse has been proved to be the smallest counterexample (in terms of the cardinality of the sample space) with three events\footnote{Stepniak, Czeslaw. "Bernstein's examples on independent events." The College Mathematics Journal 38.2 (2007): 140-142.}. In fact, it has also been shown\footnote{Stępniak, Czesław, and Tomasz Owsiany. "Are Bernstein's Examples on Independent Events Paradoxical?." Statistical Inference, Econometric Analysis and Matrix Algebra: Festschrift in Honour of Götz Trenkler (2009): 411-414.} that the minimal sample size required to existing $k$ independent events is $2^k$ and the minimal sample size for existing $k$ events each $k-1$ of them being independent is $2^{k-1}$. In other words, independent events are a luxury that only sufficiently rich sample spaces can have. 
\end{solution}
\begin{problem}{6}
    Given an example of two random variables $\xi, \eta$ which are not independent, yet $\Exp{\xi\eta} = \Exp{\xi}\Exp{\eta}$. 
\end{problem}
\begin{solution}
    Let $X$ be uniformly distributed from $[0,2\pi]$. Let $\xi = \sin(X)$ and $\eta = \cos(X)$. Clearly, $\xi$ and $\eta$ are not independent. This is because 
    \begin{align*}
        \Pr\left[\xi \in \left[\frac{\sqrt{3}}{2}, 1\right], \eta \in \left[\frac{\sqrt{3}}{2}, 1\right]\right] = 0 \neq \Pr\left[\xi \in \left[\frac{\sqrt{3}}{2}, 1\right]\right]\Pr\left[\eta \in \left[\frac{\sqrt{3}}{2}, 1\right]\right] = \frac{1}{6}.
    \end{align*}
    However, we see that $\Exp{\xi} = \Exp{\eta} = 0$ and $\Exp{\xi\eta} = \Exp{\sin(X)\cos(X)} = \Exp{\frac{1}{2}\sin(2X)} = 0$.
\end{solution}

\begin{problem}{7}
    A random variable $\xi$ has Gaussian distribution with mean zero and variance one, while a random variable $\eta$ has the distribution with density 
    \[
        p_\eta(t) = \begin{cases}te^{-\frac{t^2}{2}} &\text{if}\,t\geq 0, \\
        0 &\text{otherwise}.\end{cases}  
    \]
    Find the distribution of $\eta \cdot \xi$ assuming that $\xi$ and $\eta$ are independent.
\end{problem}
\begin{solution}
Let $z = \eta \cdot \xi$. We find the distribution function of $z$:
\begin{align*}
    F_z(x) &= \Prob{\xi \leq \frac{x}{\eta}}, \\
    &= \int_0^\infty \Prob{\xi \leq \frac{x}{y}} p_\eta(y)~dy, \\
    &= \int_0^\infty p_\eta(y) \int_{-\infty}^{x/y} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}z^2}~dzdy, \\
    &= \int_0^\infty ye^{-\frac{y^2}{2}} \Phi\left(\frac{x}{y}\right)~dy,
    \intertext{where $\Phi(\cdot)$ is the distribution function of the standard normal distribution.}
\end{align*}
\end{solution}

\begin{problem}{8}
Let $\xi_1$ and $\xi_2$ be two independent Gaussian distribution with mean zero and variance one. Prove that $\eta_1 = \xi_1^2 + \xi_2^2$ and $\eta_2 = \xi_1/\xi_2$ are independent. 
\end{problem}
\begin{solution}
    
\end{solution}

\begin{problem}{9}
    Two editors were independently proof-reading the same manuscript. One found $a$ misprints, the other found $b$ misprints. Of those, $c$ misprints were found by both of them. How would you estimate the total number of misprints in the manuscript?
\end{problem}
\begin{solution}
    Suppose that each reader finds typos independently with probability $p, q$, respectively. Let $n$ denote the total number of typos. If $n$ is sufficiently large, then $a = pn$, $b = qn$, and $c = pqn$. Then, solving for $n$, we have that $n = \frac{ab}{c}$. 
\end{solution}

\begin{problem}{10}
    Let $\xi,\eta$ be independent Poisson distributed random variables with expectations $\lambda_1,\lambda_2$, respectively. Find the distribution of $\zeta = \xi + \eta$. 
\end{problem}
\begin{solution}
    If $\eta \sim \Pois(\lambda_1)$ and $\xi \sim \Pois(\lambda_2)$, then 
    \begin{align*}
        \Pr[\zeta = a] &= \sum_{k=0}^a \Pr[\eta = k]\Pr[\xi = a - k], \\
        &= \sum_{k=0}^a \left(\frac{\lambda_1^k e^{-\lambda_1}}{k!}\right)\left(\frac{\lambda_2^{a-k}e^{-\lambda_2}}{2!(a-k)!}\right), \\
        &= e^{-(\lambda_1 + \lambda_2)}\left(\frac{\lambda_2^a}{a!} + \frac{\lambda_1\lambda_2^{a-1}}{2!(a-1)!} + \ldots + \frac{\lambda_1^{a-1}\lambda_2}{2!(a-1)!} + \frac{\lambda_1^a}{a!}\right), \\
        &= \frac{e^{-(\lambda_1 + \lambda_2)}}{a!}\left(\lambda_2^a + \lambda_1\lambda_2^{a-1}\cdot a + \lambda_1^2\lambda_2^{a-2} \cdot \frac{a(a-1)}{2} + \lambda_1^{a-2}\lambda_2^2 \cdot \frac{a(a-1)}{2} + \lambda_1^{a-1}\lambda_2 \cdot a + \lambda_1^a\right), \\
        &=  \frac{e^{-(\lambda_1 + \lambda_2)}}{a!} \sum_{k=0}^a \binom{a}{k} \lambda_1^k \lambda_2^{a-k}, \\
        &= \frac{e^{-(\lambda_1 + \lambda_2)}}{a!} \cdot (\lambda_1 + \lambda_2)^a.
    \end{align*}
    Therefore, $\zeta \sim \Pois(\lambda_1 + \lambda_2)$. Note that the first expression in the derivation is the discrete convolution between two random variables. 
\end{solution}

\begin{problem}{11}
    Let $\xi,\eta$ be independent random variables. Assume that $\xi$ has the uniform distribution on $[0,1]$, and $\eta$ has the Poisson distribution with parameter $\lambda$. Find the distribution of $\zeta = \xi + \eta$. 
\end{problem}
\begin{solution}
    Let $\Omega$ denote the space of all elementary outcomes. 
    \begin{align*}
        \Pr[\xi + \eta \leq z] &= \Pr[\{(x,y) \in \eta(\Omega) \times \xi(\Omega): x + y \leq z\}], \\
        &= \sum_{x=1}^{\floor{z}} \Pr[\eta = x]\Pr[\xi \in \{y \in \xi(\Omega): x + y \leq z\}], \\
        &= \sum_{x=1}^{\floor{z}} \frac{e^{-\lambda} \lambda^x}{x!} + \frac{e^{-\lambda}\lambda^{\floor{z}}}{\floor{z}!}\left(\frac{z - \floor{z}}{z}\right).
    \end{align*}
    The second equality follows from the law of total probability. It feels like this sum should be some gamma distribution, but it doesn't seem like there's any clean way to derive the term on the left. 
\end{solution}

\begin{problem}{12}
    Let $\xi_1,\xi_2,\ldots$ be independent identically distributed Gaussian random variables with mean zero and variance one. Let $\eta_1,\eta_2,\ldots$ be independent and identically distributed exponential random variables with mean one. Prove that there is such an $n > 0$ such that \[
        \Pr[\max(\eta_1,\ldots,\eta_n) \geq \max(\xi_1,\ldots,\xi_n)] > 0.99.
    \]
\end{problem}
\begin{solution}
    We start by estimating the expression on the left. 
    \begin{align*}
        \Pr[\max(\eta_1,\ldots,\eta_n) \geq \max(\xi_1,\ldots,\xi_n)] &= \int_0^\infty \Pr[\max(\eta_1,\ldots,\eta_n) \geq x]\Pr[\max(\xi_1,\ldots,\xi_n) \leq x]~dx, \\
        &= \int_0^\infty (1-\Pr[\max(\eta_1,\ldots,\eta_n) < x]) \Pr[\max(\xi_1,\ldots,\xi_n) \leq x]~dx, \\
        &= \int_0^\infty \left(1- \prod_{i=1}^n \Pr[\eta_i < n]\right) \prod_{i=1}^n \Pr[\xi_i \leq x]~dx, \tag{by i.i.d.}\\
        &= \int_0^\infty (1 - \Pr[\eta_1 < x]^n) \Pr[\xi_1 < x]^n~dx, \tag{by i.i.d.} \\
        &= \int_0^\infty (1 - F^n(x))\Phi^n(x)~dx, 
        \intertext{where $F(\cdot)$ is the distribution function of the exponential distribution and $\Phi(\cdot)$ is the distribution function of the standard normal distribution.}
    \end{align*}
\end{solution}

\begin{problem}{13}
    Suppose that $A_1, A_2$ are independent algebras. Prove that $\sigma(A_1), \sigma(A_2)$ are also independent.
\end{problem}
\begin{solution}
    
\end{solution}

\begin{problem}{14}
Let $\xi_1,\xi_2,\ldots$ be independent identically distributed random variables and $N$ be an $\N$-valued random variable independent of $\xi_i$'s. Show that if $\xi_1$ and $N$ have finite expectation, then \[
    \Exp{\sum_{i=1}^N \xi_i} = \Exp{N}\Exp{\xi_1}.
\]
\end{problem}
\begin{solution}
    For any $i \in \N$, let $\xi_i$ be identical random variables on the probability space $(\Omega_1, \cF_1, \mu)$. On the other hand, let $N$ be a $\N$-valued random variable on the probability space $(\Omega_2, \cF_2, \nu)$. Then,
    \begin{align*}
        \Exp{\sum_{i=1}^N \xi_i} &= \int_{\Omega_1 \times \Omega_2} \sum_{i=1}^N \xi_i~d(\mu \times \nu), \\
        &= \int_{\Omega_2}\int_{\Omega_1} \sum_{i=1}^N \xi_i~d\mu d\nu, \\
        &= \int_{\Omega_2} \sum_{i=1}^N \Exp{\xi_1}~d\nu, \\
        &= \Exp{\xi_1} \int_{\Omega_2} N~d\nu, \\
        &= \Exp{\xi_1} \Exp{N}.
    \end{align*}
\end{solution}